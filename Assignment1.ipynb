{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quamranhasan/KAIST_CS372-NLP_with_python/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOHN5SI1sXOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84a9e01-6a44-430a-fe19-05c0e1b04e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Package udhr is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "import nltk, re, pprint\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('brown')\n",
        "nltk.download('words')\n",
        "nltk.download('cmudict')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download(\"udhr\") #for problem 10\n",
        "nltk.download('punkt') #for problem 11"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "U3Zky5UrzaLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1 (2 points)**\n",
        "\n",
        "\n",
        "Write a Python code that prints out a sorted list of unique words of the sentences *sent1, ... , sent9,* using list addition and sorted operations. Also, print out the size of the sorted list of unique words. \n"
      ],
      "metadata": {
        "id": "I87Lrsrtz3YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 1\n",
        "\"\"\"\n",
        "ASSUMPTIONs: only words are being considered as words and we dont consider special characters as words\n",
        "\"\"\"\n",
        "#concatenate all the sentences and collect the set to remove the dupilcates\n",
        "unique_words_list = sorted((set(sent1 + sent2 + sent3 + sent4 + sent5 + sent6 + sent7 + sent8 + sent9)))\n",
        "#filter out non alphabetical elements to keep only words\n",
        "unique_words_list = list(filter(lambda x: x.isalpha(), unique_words_list))\n",
        "print(unique_words_list)\n",
        "print(\"Size:\", len(unique_words_list))"
      ],
      "metadata": {
        "id": "86SnMxZyz85l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c078c6e-1b38-4e69-8c21-2bca889cf491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ARTHUR', 'Call', 'Citizens', 'Dashwood', 'Fellow', 'God', 'House', 'I', 'In', 'Ishmael', 'JOIN', 'KING', 'London', 'MALE', 'PMing', 'Park', 'Pierre', 'Representatives', 'SCENE', 'SEXY', 'Saffron', 'Senate', 'Sussex', 'THE', 'The', 'Vinken', 'Whoa', 'a', 'and', 'as', 'attrac', 'been', 'beginning', 'board', 'clop', 'cloud', 'created', 'director', 'discreet', 'earth', 'encounters', 'family', 'for', 'had', 'have', 'heaven', 'in', 'join', 'lady', 'lay', 'lol', 'long', 'me', 'nonexecutive', 'of', 'old', 'older', 'on', 'people', 'problem', 'ragged', 'red', 'seeks', 'settled', 'side', 'single', 'suburb', 'sunset', 'the', 'there', 'to', 'will', 'wind', 'with', 'years']\n",
            "Size: 75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2 (2 points)**\n",
        "\n",
        "Write a Python code that extracts the last two words of *text2* using the slice expressions.\n"
      ],
      "metadata": {
        "id": "H3fxRCWSz6Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 2\n",
        "print(text2[-2:])"
      ],
      "metadata": {
        "id": "L2WRhN3t0EEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53a9f8f-2e56-421c-8b7c-2e3c0bbf1601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['THE', 'END']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3 (4 points)**\n",
        "Define a Python function called *vocab_size(text)* that has a single parameter for the text, and returns the vocabulary size of the input text. As an input text, please use a list of words that concatenates *sent1, sent2, ..., sent9*. Report the vocab size after counting the words in their original state and changing them all to lowercase. \n"
      ],
      "metadata": {
        "id": "ReMNzsql0G6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 3\n",
        "\n",
        "#vocab size = all unique words\n",
        "def vocab_size(text):\n",
        "    return len(set(text))\n",
        "\n",
        "list_of_words = sent1+ sent2+ sent3+ sent4+ sent5+ sent6+ sent7+ sent8+ sent9\n",
        "print(\"vocab size in Original:\", vocab_size(list_of_words))\n",
        "\n",
        "in_lower_case = [w.lower() for w in list_of_words]\n",
        "print(\"vocab size in lowercase:\", vocab_size(in_lower_case))"
      ],
      "metadata": {
        "id": "jXki8mjq0WEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c5abce-bd9d-423c-ba51-23f9561d14e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size in Original: 87\n",
            "vocab size in lowercase: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 4 (4 points)**\n",
        "\n",
        "Assume that a list of words ['cheek', 'camera', 'character', 'child', 'cider', 'china', 'cute', 'chrome'] is given. Write a Python code to perform the following tasks: \n",
        "\n",
        "a. Print all words beginning with “*ch*”. (2 points)\n",
        "\n",
        "b. Print all words longer than five characters. (2 points)\n"
      ],
      "metadata": {
        "id": "QZOsuVpz0aDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 4\n",
        "word_list = ['cheek', 'camera', 'character', 'child', 'cider', 'china', 'cute', 'chrome']\n",
        "\n",
        "words_beginning_with_ch = [w for w in word_list if w.startswith('ch')]\n",
        "print(f'words_beginning_with_ch: {words_beginning_with_ch}')\n",
        "\n",
        "words_more_than_5_chars = [w for w in word_list if len(w)>5]\n",
        "print(f'words_more_than_5_chars: {words_more_than_5_chars}')\n"
      ],
      "metadata": {
        "id": "FaW534DP0jxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3410eaa-b141-4c30-88ad-7e998242eeb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words_beginning_with_ch: ['cheek', 'character', 'child', 'china', 'chrome']\n",
            "words_more_than_5_chars: ['camera', 'character', 'chrome']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 5 (3 points)**\n",
        "\n",
        "We are trying to check out some special bigrams that appear in the same sentence with two words between them. (for example, the sentence “I am a little boy.” has two pairs of the special bigrams (I, little) and (am, boy).) Retrieve 10 most frequent special bigrams in the Brown Corpus News category and show the results.\n"
      ],
      "metadata": {
        "id": "dOUi8BqB08CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 5\n",
        "#load words\n",
        "words = nltk.corpus.brown.words(categories='news')\n",
        "\n",
        "# create a list of special bigrams that appear in the same sentence with two words between them\n",
        "#we use len-3 because we are searching for groups of 4 words --> to_be_collected_word, _____, _____, to_be_collected_word2\n",
        "bigrams = [(words[i], words[i+3]) for i in range(len(words)-3)]\n",
        "\n",
        "# get the 10 most frequent bigrams\n",
        "freq_dist = nltk.FreqDist(bigrams)\n",
        "top_10 = freq_dist.most_common(10)\n",
        "\n",
        "# print the results\n",
        "for bigram, freq in top_10:\n",
        "    print(f\"({bigram[0]} {bigram[1]}) count:{freq}\")"
      ],
      "metadata": {
        "id": "-Nqx3_tx07jE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec1fb36-1dce-47b0-a2a8-a2139ed4f653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(, ,) count:382\n",
            "(the the) count:379\n",
            "(the .) count:351\n",
            "(the ,) count:267\n",
            "(the of) count:263\n",
            "(, the) count:219\n",
            "(. ,) count:212\n",
            "(of ,) count:203\n",
            "(of .) count:201\n",
            "(. the) count:199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 6 (3 points)**\n",
        "\n",
        "Show how many words there are in the Gutenberg shakespeare-macbeth corpus that have two or more pronunciations considering the CMU Pronouncing Dictionary. Also, show the word that has the most diverge pronunciations.\n"
      ],
      "metadata": {
        "id": "z2MHKTnN1Eld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 6\n",
        "macbeth_words = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
        "cmu_pronouncing_dict = nltk.corpus.cmudict.dict()\n",
        "\n",
        "#get macbeth words with two or more pronunciations in the cmu dictionary and then collect the set of the words. \n",
        "words_with_multiple_pronunciations = set(word.lower() for word in macbeth_words if word.lower() in cmu_pronouncing_dict and len(cmu_pronouncing_dict[word.lower()]) >=2)\n",
        "print(f\"Number of words with two or more pronunciations pronunciations:{len(words_with_multiple_pronunciations)} \")\n",
        "\n",
        "\n",
        "#initialize a variable to keep count of the max number of pronunciations\n",
        "\n",
        "max_num_prons = 0\n",
        "#for each of the word with multiple pronounciations we find number of unique pronounciations and if it is higher than the max_num_pronuns we update the value of max_num_prons with that value\n",
        "for word in words_with_multiple_pronunciations:\n",
        "    pronunciations = cmu_pronouncing_dict[word.lower()]\n",
        "    unique_pronunciations = set()\n",
        "    for pron in pronunciations:\n",
        "        unique_pronunciations.update(pron)\n",
        "\n",
        "    num_pronunciations = len(unique_pronunciations)\n",
        "    if num_pronunciations > max_num_prons:\n",
        "        most_divergent_word = word\n",
        "        max_num_prons = num_pronunciations\n",
        "\n",
        "# print the result\n",
        "print(\"Word with most diverge pronunciation is: \"+most_divergent_word +\", Number of pronunciations: \" +str(max_num_prons))\n",
        "\n"
      ],
      "metadata": {
        "id": "Fkj9fJYW1DzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788021b5-41ed-4791-b7bf-975726efe86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words with two or more pronunciations pronunciations:304 \n",
            "Word with most diverge pronunciation is: direction, Number of pronunciations: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 7 (4 points)**\n",
        "\n",
        "\n",
        "Write a brief explanation for both of the following questions.\n",
        "\n",
        "a. Get all ‘duck’ noun synsets from wordnet. From the synsets print all hypernyms. Also, print all lemma names from the hypernyms. (2 points)\n",
        "\n",
        "b. Using wn.all_synsets(‘v’), write a code that prints 5 random lemmas from the first 10 verb synsets. (the output lemma will be something like ‘Lemma('run.v.02.run')’) (2 points)\n"
      ],
      "metadata": {
        "id": "ecuw59je1I6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 7\n",
        "\"\"\"PART A:\n",
        "  To get all 'duck' noun synsets from WordNet,\n",
        "  import the 'wordnet' module from the 'nltk' package \n",
        "  use the 'synsets' method to retrieve all synsets for 'duck' as a noun. \n",
        "  We can then use the 'hypernyms' method to get all hypernyms\n",
        "  for each synset and print their lemma names.\n",
        "\"\"\"\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "# Get all 'duck' noun synsets\n",
        "duck_noun_synsets = wn.synsets('duck', pos='n')\n",
        "print(f\"all_duck_noun_synsets:  {duck_noun_synsets}\")\n",
        "#new line for spacing\n",
        "print()\n",
        "\n",
        "# Print all hypernyms and lemma names\n",
        "for synset in duck_noun_synsets:\n",
        "    print('Hypernyms for', synset.name(), ':')\n",
        "    for hypernym in synset.hypernyms():\n",
        "        print(f\"\\thypernyms: {hypernym}\")\n",
        "        print(f\"\\tlemma names from hypernym: {hypernym.lemma_names()}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "\"\"\"PART B\n",
        "We pass verb tag as an argument to filter synsets for verbs and slice the resulting list to only keep the first 10\n",
        "use the 'random.choice' function to randomly select synsets, \n",
        "and then use the 'lemmas' method to retrieve lemmas from each synset.\n",
        "until we have 5 lemmas \n",
        "\"\"\"\n",
        "import random\n",
        "import nltk\n",
        "\n",
        "# Get the first 10 verb synsets\n",
        "verb_synsets = list(wn.all_synsets('v'))[:10]\n",
        "\"\"\"\n",
        "ASSUMPTIONS:\n",
        "we choose 5 random lemmas from the first 10 synsets\n",
        "\"\"\"\n",
        "selected_lemmas = []\n",
        "while len(selected_lemmas) < 5:\n",
        "    synset = random.choice(verb_synsets)\n",
        "    lemma = random.choice(synset.lemmas())\n",
        "    #check if the lemma is not already in the list\n",
        "    if lemma not in selected_lemmas:\n",
        "        selected_lemmas.append(lemma)\n",
        "\n",
        "# Print the selected lemmas\n",
        "print(\"Selected lemmas:\")\n",
        "for lemma in selected_lemmas:\n",
        "    print(lemma)"
      ],
      "metadata": {
        "id": "U1wt6adZ1YWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22fd6a88-e71d-4adb-96d2-b915800c2b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_duck_noun_synsets:  [Synset('duck.n.01'), Synset('duck.n.02'), Synset('duck.n.03'), Synset('duck.n.04')]\n",
            "\n",
            "Hypernyms for duck.n.01 :\n",
            "\thypernyms: Synset('anseriform_bird.n.01')\n",
            "\tlemma names from hypernym: ['anseriform_bird']\n",
            "\n",
            "Hypernyms for duck.n.02 :\n",
            "\thypernyms: Synset('score.n.03')\n",
            "\tlemma names from hypernym: ['score']\n",
            "\n",
            "Hypernyms for duck.n.03 :\n",
            "\thypernyms: Synset('poultry.n.02')\n",
            "\tlemma names from hypernym: ['poultry']\n",
            "\n",
            "Hypernyms for duck.n.04 :\n",
            "\thypernyms: Synset('fabric.n.01')\n",
            "\tlemma names from hypernym: ['fabric', 'cloth', 'material', 'textile']\n",
            "\n",
            "Selected lemmas:\n",
            "Lemma('respire.v.01.respire')\n",
            "Lemma('hyperventilate.v.01.hyperventilate')\n",
            "Lemma('aspirate.v.03.aspirate')\n",
            "Lemma('hiccup.v.01.hiccup')\n",
            "Lemma('force_out.v.08.force_out')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 8 (3 points)**\n",
        "\n",
        "Describe the class of strings matched by the following regular expressions.\n",
        "1.\t[a-zA]+\n",
        "2.\t[a-z][A-Z]*\n",
        "3.\t[BGTC]r[aeiou]{,2}t\n",
        "4.\t([aeiou][^aeiou])*\n",
        "5.\t\\W+|[^\\W\\s]+\n",
        "\n",
        "You can test your answers using nltk.re_show().\n"
      ],
      "metadata": {
        "id": "_FvQLJEM1aI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 8 (for this problem, the answers should be in the report)\n",
        "\"\"\"Answers are also in the report\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "1: The regex matches strings containing one or multiple instances of \"a-z\" or \"A\" or both. \n",
        "example strings: \n",
        "bbW: will match {bb} \n",
        "bbA: will match {bbA}\n",
        "23W: WONT MATCH \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "2: The regex string matches strings containing a lowercase letter or a lowercase letter that is followed by 0 or more uppercase letters.\n",
        "example strings: \n",
        "bbW: will match {b} and {bW} \n",
        "b2W: will match only {b}\n",
        "222: WONT MATCH \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "3: The regex string matches strings containing any one of \"BGTC\" followed by \"r\" followed by 0 to 2 vowels and ending with a \"t\"\n",
        "example strings: \n",
        "Great: will match {Great} \n",
        "reTreat: will match {Treat}\n",
        "Greeeet: WONT MATCH \n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "4: The regex string matches strings containing zero or more instances vowel(in lower case) and followed by a non-vowel(including A,E,I,O,U). It will also match empty strings\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "5: Lets break down the regex:\n",
        "\\W+ matches one or more instances any non-word character\n",
        "| means OR\n",
        "[^\\W\\s]+ matches one or more instances of NOT non-words and NOT (white spaces, TABS or new line)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "pattern = r\"[BGTC]r[aeiou]{,2}t\"\n",
        "text = \"Great\"\n",
        "\n",
        "nltk.re_show(pattern, text)"
      ],
      "metadata": {
        "id": "rrkrUCOV1iQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed542295-5177-4880-e1da-07df117591d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{Great}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 9 (5 points)**\n",
        "\n",
        "Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of a word to the end, then append ay, e.g., string → ingstray, STRING-> INGSTRAY, idle → idleay (For further details of Pig Latin, please refer to http://en.wikipedia.org/wiki/Pig_Latin). \n",
        "\n",
        "This time we are going to add some rules to Pig Latin. After moving the consonant of the word, we will change the first vowel in the word. If the vowel is ‘a’ change it to ‘e’, if ‘e’ change it to ‘i’, if ‘i’ change it to ‘o’, if ‘o’ change it to ‘u’, and for ‘u’ change it to ‘a’. After that we append ay. e.g. string → ongstray, idle -> odleay\n",
        "\n",
        "a. Write a Python function to convert a word to its equivalent in the new Pig Latin. (3 points)\n",
        "\n",
        "b. Write a Python code that converts a text (sequence of words), instead of individual words.  (1 points)\n"
      ],
      "metadata": {
        "id": "nNet6GaG10Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 9\n",
        "\n",
        "\"\"\"\n",
        "PROBLEM A\n",
        "ASSUMPTIONS: inputs are non empty strings and inputs all are alphabets and not punctuation marks\n",
        "\"\"\"\n",
        "def piggy(word):\n",
        "  #list of vowels\n",
        "  vowels = 'aeiouAEIOU'\n",
        "  cons = \"\"\n",
        "\n",
        "  #two different endings to cater for both UPPERCASE and lowercase scenarios\n",
        "  end_l= \"ay\"\n",
        "  end_u = \"AY\"\n",
        "\n",
        "  #start collecting letter until the first occurence of a vowel.\n",
        "  i = 0\n",
        "  while i < len(word):\n",
        "    if word[i] in vowels:\n",
        "      break\n",
        "    cons += word[i]\n",
        "    i += 1\n",
        "\n",
        "  #put the remainder of the word and append the part before the instance of the vowel\n",
        "  word = word[i:]+cons #results in ingstr\n",
        "\n",
        "  #change the vowel as indicated in the question and append the rest of the word\n",
        "  if(word[0] == \"a\"):\n",
        "    word = \"e\"+word[1:]\n",
        "\n",
        "  elif(word[0] == \"e\"):\n",
        "    word = \"i\"+word[1:]\n",
        "\n",
        "  elif(word[0] == \"i\"):\n",
        "    word = \"o\"+word[1:]\n",
        "\n",
        "  elif(word[0] == \"o\"):\n",
        "    word = \"u\"+word[1:]\n",
        "\n",
        "  elif(word[0] == \"u\"):\n",
        "    word = \"a\"+word[1:]\n",
        "\n",
        "  elif(word[0] == \"A\"):\n",
        "     word = \"E\"+word[1:]\n",
        "\n",
        "  elif(word[0] == \"E\"):\n",
        "    word = \"I\"+word[1:]\n",
        "\n",
        "  elif(word[0] == \"I\"):\n",
        "    word = \"O\"+word[1:]\n",
        "\n",
        "  elif(word[0] == \"O\"):\n",
        "    word = \"U\"+word[1:]\n",
        "\n",
        "  elif(word[0] == \"U\"):\n",
        "    word = \"A\"+word[1:]\n",
        "\n",
        "\n",
        "#IF WE HAVE ALL Uppercase starting character we append \"AY\" following the\n",
        "  if(word[0].isupper()):\n",
        "    word = word +end_u\n",
        "\n",
        "  else:\n",
        "    word = word +end_l\n",
        "  return word\n",
        "\n",
        "piggy(\"IDLE\")\n",
        "\n"
      ],
      "metadata": {
        "id": "irIbiO6U19Mc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f3749d89-a166-4658-df43-7498331ce046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ODLEAY'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "PROBLEM B\n",
        "I am considernig each word in the sentence separately and passing them in the function and then joining the results.\n",
        "\"\"\"\n",
        "\n",
        "def pig_latin_sent(sentence):\n",
        "  # split the sentences into words.\n",
        "  words = sentence.split()\n",
        "\n",
        "  answer_words = []\n",
        "  for word in words:\n",
        "    #For each of the words in the sentence we pass the word as an argument to the function \n",
        "    answer_word = piggy(word)\n",
        "    #store the output in the “answer_words”\n",
        "    answer_words.append(answer_word)\n",
        "  return \" \".join(answer_words)\n",
        "  \n",
        "print(pig_latin_sent(\"hey wassup\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzwQoXzCtj4z",
        "outputId": "0a3edf9d-0cd4-42f3-c0d7-c9a160634e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iyhay essupway\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 10 (4 points)**\n",
        "\n",
        "Using a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), along with NLTK’s frequency distribution (nltk.FreqDist), develop a system that guesses the language of a previously unseen text. Also, you can guess the language of the unseen text based on comparison between how often each word is used in each language in the corpus and how often each word is used in unseen text. To this end, you can use the rank correlation functionality (nltk.spearman_correlation). For simplicity, you can work with a single character encoding (not at word-level) and just a few, and at least two, languages in your favor. \n"
      ],
      "metadata": {
        "id": "j9tST3r52Dhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 10\n",
        "import nltk\n",
        "from nltk.corpus import udhr\n",
        "\n",
        "\n",
        "def find_lang(text):\n",
        "  # english and french\n",
        "  our_langs = ['English-Latin1', 'French_Francais-Latin1'] \n",
        "  # Build frequency distributions for each language in the corpus\n",
        "  freq_dists = {}\n",
        "  for lang in our_langs:\n",
        "      corpus = udhr.words(lang)\n",
        "      freq_dists[lang] = nltk.FreqDist(corpus)\n",
        "\n",
        "\n",
        "  #Build a frequency distribution for the unseen text\n",
        "  text_freq_dist = nltk.FreqDist(text)\n",
        "\n",
        "  #Spearman correlation between the frequency distributions for each language and the unseen text\n",
        "  correlations = {}\n",
        "  for lang in our_langs:\n",
        "      lang_freq_dist = freq_dists[lang]\n",
        "      rank_correlation = nltk.spearman_correlation(lang_freq_dist, text_freq_dist)\n",
        "      correlations[lang] = rank_correlation\n",
        "\n",
        "  # Determine the language with the highest correlation\n",
        "  best_guess = max(correlations, key=correlations.get)\n",
        "  return f\"language guess: {best_guess}\"\n",
        "\n",
        "print(find_lang(\"Bonjour, comment vas-tu\"))"
      ],
      "metadata": {
        "id": "HPyeLRM_2BKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c98838e-501a-412e-ccb0-5d813df5683a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "language guess: French_Francais-Latin1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 11 (6 points)**\n",
        "\n",
        "Write a Python program that processes a text and discovers cases where a word has been used with a novel sense. One simple approach is using WordNet: For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context. Please note that this is a crude approach. We also notice that doing it well is a difficult, open research problem. Please define what a “novel sense” of the word is in your own words, and describe the detailed design choice in your implementation.\n"
      ],
      "metadata": {
        "id": "pNwK-R-Z2Nnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for problem 11\n",
        "\"\"\"\n",
        "A novel sense of the word means that the word has not been used before in that sense and hence is not inlcuded in dictionaries\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def novel_sense(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    #print(words)\n",
        "    for word in words:\n",
        "        # Get synsets of the word\n",
        "        synsets = wn.synsets(word)\n",
        "        # for every synset get the similarity between synset andother synsets\n",
        "        for synset in synsets:\n",
        "            similarity_scores = []\n",
        "            for context_word in words:\n",
        "                #we skip the scenarios when the words are the same. This is for avoiding the 1.0 similarity scores. \n",
        "                if(context_word == word): \n",
        "                  continue\n",
        "\n",
        "                context_synsets = wn.synsets(context_word)\n",
        "                for context_synset in context_synsets:\n",
        "                    similarity = synset.path_similarity(context_synset)\n",
        "\n",
        "                    #if a connecting path exists\n",
        "                    if similarity is not None:\n",
        "                        similarity_scores.append(similarity)\n",
        "            # If the maximum similarity score is below 0.1,we consider it a novel sense\n",
        "            #print(similarity_scores)\n",
        "            if max(similarity_scores) < 0.1:\n",
        "                return (f\"Possible novel sense of '{word}' in the text: {synset.definition()}.\")\n",
        "  \n",
        "print(novel_sense(\"this drink tastes heavenly\"))"
      ],
      "metadata": {
        "id": "ZY2Xrs2j2UON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbef8af4-1bbd-4fe2-c27e-93ecfe0f201b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible novel sense of 'tastes' in the text: the sensation that results when taste buds in the tongue and throat convey information about the chemical composition of a soluble stimulus.\n"
          ]
        }
      ]
    }
  ]
}