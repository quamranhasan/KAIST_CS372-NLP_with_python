{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quamranhasan/KAIST_CS372-NLP_with_python/blob/main/Assignment3_CS372.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google-research-datasets/gap-coreference"
      ],
      "metadata": {
        "id": "vfKYU8Hr7jyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fdfae8-3164-4145-93ad-72ec51c6733f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gap-coreference'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Total 19 (delta 0), reused 0 (delta 0), pack-reused 19\u001b[K\n",
            "Unpacking objects: 100% (19/19), 970.66 KiB | 4.60 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data file by the following code."
      ],
      "metadata": {
        "id": "BVC6mGRm8XEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import csv\n",
        "from enum import Enum\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rKz3wECu9C4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Develop your program that resolves ambiguous pronouns that refer to names."
      ],
      "metadata": {
        "id": "-_jbkp0l8npy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "'''\n",
        "insert three spaces at each offset position in the string and return the modified string\n",
        "the spaces are placed in front of the words we want to analyze. (name1, name2, ambiguous pronoun)\n",
        "'''\n",
        "def mark_string(string, offset_list):\n",
        "    for offset in offset_list:\n",
        "        string = string[:offset] + '   ' + string[offset:]\n",
        "    return string\n",
        "\n",
        "\n",
        "'''\n",
        "Function for switching to page-context.\n",
        "searches if the given names are in the url\n",
        "if there are spaces replace those with underscores.\n",
        "'''\n",
        "def name_is_in_url(name_1, name_2, url):\n",
        "    mod_name_1 = name_1.replace(' ', '_')\n",
        "    mod_name_2 = name_2.replace(' ', '_')\n",
        "\n",
        "    if name_1 in url:\n",
        "        return 'a'\n",
        "    elif mod_name_1 in url and ' ' in name_1:\n",
        "        return 'a'\n",
        "    elif name_2 in url:\n",
        "        return 'b'\n",
        "    elif mod_name_2 in url and ' ' in name_2:\n",
        "        return 'b'\n",
        "\n",
        "\n",
        "'''\n",
        "tokenize mark_string with nlp\n",
        "identify pronoun from name_list and its dependency relationship in the tokenized string\n",
        "compare the dependency and determine if find_parallelism exists\n",
        "return none if no find_parallelism\n",
        "'''\n",
        "def find_parallelism(string, offs_list, name_list):\n",
        "    string_used = mark_string(string, offs_list)\n",
        "    doc = nlp(string_used)\n",
        "    pronoun = name_list[2]\n",
        "    pronoun_dep = ''\n",
        "    a, b = '', ''\n",
        "\n",
        "    for token in doc:\n",
        "        if token.text == pronoun or token.text in pronoun:\n",
        "            pronoun_dep = token.dep_\n",
        "            break\n",
        "\n",
        "    for token in doc:\n",
        "        if token.text == name_list[0] or token.text in name_list[0]:\n",
        "            a += token.dep_\n",
        "        elif token.text == name_list[1] or token.text in name_list[1]:\n",
        "            b += token.dep_\n",
        "\n",
        "    if a == pronoun_dep:\n",
        "        return 'a'\n",
        "    elif b == pronoun_dep:\n",
        "        return 'b'\n",
        "\n",
        "\n",
        "'''\n",
        "extract features from data\n",
        "return a dictionary with extracted features\n",
        "\n",
        "'''\n",
        "def get_features(row):\n",
        "    selected_columns = ['A-offset', 'Pronoun-offset', 'B-offset']\n",
        "    column_lists = []\n",
        "    for column in selected_columns:\n",
        "        column_list = row[column]\n",
        "        column_lists.append(column_list)\n",
        "\n",
        "\n",
        "    selected_columns2 = ['A', 'B', 'Pronoun']\n",
        "    column_lists2 = []\n",
        "    for column in selected_columns2:\n",
        "        column_list2 = row[column]\n",
        "        column_lists2.append(column_list2)\n",
        "\n",
        "    return {\n",
        "        #comment first one if you want to try snippet context\n",
        "        #keep both if you want to try page-context\n",
        "        'name_is_in_url': name_is_in_url(row['A'], row['B'], row['URL']),\n",
        "        'find_parallelism': find_parallelism(row['Text'], column_lists,column_lists2)\n",
        "    }\n",
        "\n",
        "train = pd.read_csv('/content/gap-coreference/gap-development.tsv', sep='\\t')\n",
        "test = pd.read_csv('/content/gap-coreference/gap-test.tsv', sep='\\t')\n",
        "\n",
        "train_feature = [(get_features(row), row['A-coref']) for _, row in train.iterrows()]\n",
        "\n",
        "\n",
        "#naive bayes classifier that we learnt in class\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_feature)\n",
        "print(classifier.show_most_informative_features())\n",
        "\n",
        "final = []\n",
        "for _, row in test.iterrows():\n",
        "    classify = str(classifier.classify(get_features(row)))\n",
        "    if classify == \"True\":\n",
        "        final.append([row['ID'], 'TRUE', 'FALSE'])\n",
        "    else:\n",
        "        final.append([row['ID'], 'FALSE', 'TRUE'])\n"
      ],
      "metadata": {
        "id": "54F4j0Dq7IQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c3968d-e7f1-40bc-dde3-8bccc308c852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "        find_parallelism = 'b'             False : True   =      5.0 : 1.0\n",
            "          name_is_in_url = 'a'              True : False  =      4.5 : 1.0\n",
            "        find_parallelism = 'a'              True : False  =      3.3 : 1.0\n",
            "          name_is_in_url = 'b'             False : True   =      3.3 : 1.0\n",
            "          name_is_in_url = None            False : True   =      1.3 : 1.0\n",
            "        find_parallelism = None            False : True   =      1.0 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the results into a tsv file. The tsv file should have the test ID, True or False for A-coref and B-coref."
      ],
      "metadata": {
        "id": "vEtwJ-dW8pmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code. You may fix the code to save your answers.\n",
        "'''writing to tsv'''\n",
        "with open('result.tsv', 'w', newline = '') as out_file:\n",
        "    writer = csv.writer(out_file, delimiter='\\t')\n",
        "    for i in final:\n",
        "        writer.writerow(i)"
      ],
      "metadata": {
        "id": "jP3a3joT8ycw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for evaluation. Do not modify this."
      ],
      "metadata": {
        "id": "oXdh8wpB9Qmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do not modify this part\n",
        "class Gender(Enum):\n",
        "  UNKNOWN = 0\n",
        "  MASCULINE = 1\n",
        "  FEMININE = 2\n",
        "\n",
        "\n",
        "# Mapping of (lowercased) pronoun form to gender value. Note that reflexives\n",
        "# are not included in GAP, so do not appear here.\n",
        "PRONOUNS = {\n",
        "    'she': Gender.FEMININE,\n",
        "    'her': Gender.FEMININE,\n",
        "    'hers': Gender.FEMININE,\n",
        "    'he': Gender.MASCULINE,\n",
        "    'his': Gender.MASCULINE,\n",
        "    'him': Gender.MASCULINE,\n",
        "}\n",
        "\n",
        "# Fieldnames used in the gold dataset .tsv file.\n",
        "GOLD_FIELDNAMES = [\n",
        "    'ID', 'Text', 'Pronoun', 'Pronoun-offset', 'A', 'A-offset', 'A-coref', 'B',\n",
        "    'B-offset', 'B-coref', 'URL'\n",
        "]\n",
        "\n",
        "# Fieldnames expected in system output .tsv files.\n",
        "SYSTEM_FIELDNAMES = ['ID', 'A-coref', 'B-coref']\n",
        "\n",
        "class Annotation(object):\n",
        "  \"\"\"Container class for storing annotations of an example.\n",
        "  Attributes:\n",
        "    gender(None): The gender of the annotation. None indicates that gender was\n",
        "      not determined for the given example.\n",
        "    name_a_coref(None): bool reflecting whether Name A was recorded as\n",
        "      coreferential with the target pronoun for this example. None indicates\n",
        "      that no annotation was found for the given example.\n",
        "    name_b_coref(None): bool reflecting whether Name B was recorded as\n",
        "      coreferential with the target pronoun for this example. None indicates\n",
        "      that no annotation was found for the given example.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.gender = None\n",
        "    self.name_a_coref = None\n",
        "    self.name_b_coref = None\n",
        "\n",
        "\n",
        "class Scores(object):\n",
        "  \"\"\"Container class for storing scores, and generating evaluation metrics.\n",
        "  Attributes:\n",
        "    true_positives: Tally of true positives seen.\n",
        "    false_positives: Tally of false positives seen.\n",
        "    true_negatives: Tally of true negatives seen.\n",
        "    false_negatives: Tally of false negatives seen.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.true_positives = 0\n",
        "    self.false_positives = 0\n",
        "    self.true_negatives = 0\n",
        "    self.false_negatives = 0\n",
        "\n",
        "  def recall(self):\n",
        "    \"\"\"Calculates recall based on the observed scores.\n",
        "    Returns:\n",
        "      float, the recall.\n",
        "    \"\"\"\n",
        "    numerator = self.true_positives\n",
        "    denominator = self.true_positives + self.false_negatives\n",
        "    return 100.0 * numerator / denominator if denominator else 0.0\n",
        "\n",
        "  def precision(self):\n",
        "    \"\"\"Calculates precision based on the observed scores.\n",
        "    Returns:\n",
        "      float, the precision.\n",
        "    \"\"\"\n",
        "    numerator = self.true_positives\n",
        "    denominator = self.true_positives + self.false_positives\n",
        "    return 100.0 * numerator / denominator if denominator else 0.0\n",
        "\n",
        "  def f1(self):\n",
        "    \"\"\"Calculates F1 based on the observed scores.\n",
        "    Returns:\n",
        "      float, the F1 score.\n",
        "    \"\"\"\n",
        "    recall = self.recall()\n",
        "    precision = self.precision()\n",
        "\n",
        "    numerator = 2 * precision * recall\n",
        "    denominator = precision + recall\n",
        "    return numerator / denominator if denominator else 0.0\n",
        "\n",
        "\n",
        "def read_annotations(filename, is_gold):\n",
        "  \"\"\"Reads coreference annotations for the examples in the given file.\n",
        "  Args:\n",
        "    filename: Path to .tsv file to read.\n",
        "    is_gold: Whether or not we are reading the gold annotations.\n",
        "  Returns:\n",
        "    A dict mapping example ID strings to their Annotation representation. If\n",
        "    reading gold, 'Pronoun' field is used to determine gender.\n",
        "  \"\"\"\n",
        "\n",
        "  def is_true(value):\n",
        "    if value.lower() == 'true':\n",
        "      return True\n",
        "    elif value.lower() == 'false':\n",
        "      return False\n",
        "    else:\n",
        "      print('Unexpected label!', value)\n",
        "      return None\n",
        "\n",
        "  fieldnames = GOLD_FIELDNAMES if is_gold else SYSTEM_FIELDNAMES\n",
        "\n",
        "  annotations = defaultdict(Annotation)\n",
        "  with open(filename, 'rU') as f:\n",
        "    reader = csv.DictReader(f, fieldnames=fieldnames, delimiter='\\t')\n",
        "\n",
        "    # Skip the header line in the gold data\n",
        "    if is_gold:\n",
        "      next(reader, None)\n",
        "\n",
        "    for row in reader:\n",
        "      example_id = row['ID']\n",
        "      if example_id in annotations:\n",
        "        print('Multiple annotations for', example_id)\n",
        "        continue\n",
        "\n",
        "      annotations[example_id].name_a_coref = is_true(row['A-coref'])\n",
        "      annotations[example_id].name_b_coref = is_true(row['B-coref'])\n",
        "      if is_gold:\n",
        "        gender = PRONOUNS.get(row['Pronoun'].lower(), Gender.UNKNOWN)\n",
        "        assert gender != Gender.UNKNOWN, row\n",
        "        annotations[example_id].gender = gender\n",
        "  return annotations\n",
        "\n",
        "\n",
        "def calculate_scores(gold_annotations, system_annotations):\n",
        "  \"\"\"Score the system annotations against gold.\n",
        "  Args:\n",
        "    gold_annotations: dict from example ID to its gold Annotation.\n",
        "    system_annotations: dict from example ID to its system Annotation.\n",
        "  Returns:\n",
        "    A dict from gender to a Scores object for that gender. None is used to\n",
        "      denote no specific gender, i.e. overall scores.\n",
        "  \"\"\"\n",
        "  scores = {}\n",
        "  for example_id, gold_annotation in gold_annotations.items():\n",
        "    system_annotation = system_annotations[example_id]\n",
        "\n",
        "    name_a_annotations = [\n",
        "        gold_annotation.name_a_coref, system_annotation.name_a_coref\n",
        "    ]\n",
        "    name_b_annotations = [\n",
        "        gold_annotation.name_b_coref, system_annotation.name_b_coref\n",
        "    ]\n",
        "    for gender in [None, gold_annotation.gender]:\n",
        "      if gender not in scores:\n",
        "        scores[gender] = Scores()\n",
        "\n",
        "      for (gold, system) in [name_a_annotations, name_b_annotations]:\n",
        "        if system is None:\n",
        "          print('Missing output for', example_id)\n",
        "          scores[gender].false_negatives += 1\n",
        "        elif gold and system:\n",
        "          scores[gender].true_positives += 1\n",
        "        elif not gold and system:\n",
        "          scores[gender].false_positives += 1\n",
        "        elif not gold and not system:\n",
        "          scores[gender].true_negatives += 1\n",
        "        elif gold and not system:\n",
        "          scores[gender].false_negatives += 1\n",
        "  return scores\n",
        "\n",
        "\n",
        "def make_scorecard(scores):\n",
        "  \"\"\"Returns a human-readable scorecard of the given scores.\n",
        "  Args:\n",
        "    scores: dict from gender to its Scores object. None is used to denote no\n",
        "      specific gender, i.e. overall scores.\n",
        "  Returns:\n",
        "    A string, the scorecard.\n",
        "  \"\"\"\n",
        "  scorecard = []\n",
        "\n",
        "  display_names = [(None, 'Overall'), (Gender.MASCULINE, 'Masculine'),\n",
        "                   (Gender.FEMININE, 'Feminine')]\n",
        "\n",
        "  bias_terms = {}\n",
        "  for gender, display_name in display_names:\n",
        "    gender_scores = scores.get(gender, Scores())\n",
        "\n",
        "    recall = gender_scores.recall()\n",
        "    precision = gender_scores.precision()\n",
        "    f1 = gender_scores.f1()\n",
        "    bias_terms[gender] = f1\n",
        "\n",
        "    scorecard.append('{} recall: {:.1f} precision: {:.1f} f1: {:.1f}'.format(\n",
        "        display_name, recall, precision, f1))\n",
        "    scorecard.append('\\t\\ttp {:d}\\tfp {:d}'.format(\n",
        "        gender_scores.true_positives, gender_scores.false_positives))\n",
        "    scorecard.append('\\t\\tfn {:d}\\ttn {:d}'.format(\n",
        "        gender_scores.false_negatives, gender_scores.true_negatives))\n",
        "\n",
        "  bias = '-'\n",
        "  if bias_terms[Gender.MASCULINE] and bias_terms[Gender.FEMININE]:\n",
        "    bias = '{:.2f}'.format(\n",
        "        bias_terms[Gender.FEMININE] / bias_terms[Gender.MASCULINE])\n",
        "  scorecard.append('Bias (F/M): {}\\n'.format(bias))\n",
        "  return '\\n'.join(scorecard)\n",
        "\n",
        "\n",
        "def run_scorer(gold_tsv, system_tsv):\n",
        "  \"\"\"Run the scorer.\n",
        "  Args:\n",
        "    gold_tsv: Gold annotations to score against.\n",
        "    system_tsv: System output to score.\n",
        "  Returns:\n",
        "    A string, the scorecard.\n",
        "  \"\"\"\n",
        "  gold_annotations = read_annotations(gold_tsv, is_gold=True)\n",
        "  assert gold_annotations, 'No gold annotations read!'\n",
        "\n",
        "  system_annotations = read_annotations(system_tsv, is_gold=False)\n",
        "  assert system_annotations, 'No system annotations read!'\n",
        "\n",
        "  scores = calculate_scores(gold_annotations, system_annotations)\n",
        "  return make_scorecard(scores)"
      ],
      "metadata": {
        "id": "owZgD31k9KHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_tsv = '/content/gap-coreference/gap-test.tsv'\n",
        "system_tsv = '/content/result.tsv'"
      ],
      "metadata": {
        "id": "KGC48MngMpo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the file paths.\n",
        "\n",
        "gold_tsv = the test data\n",
        "\n",
        "system_tsv = your answer"
      ],
      "metadata": {
        "id": "CA74spYq8yC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check your results here. You may modify this part if you want to."
      ],
      "metadata": {
        "id": "e8vK5pQrtYMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorecard = run_scorer(gold_tsv, system_tsv)\n",
        "print(scorecard)"
      ],
      "metadata": {
        "id": "pB-XJjQf-Y4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49fa765-85ca-4674-86af-38acb089dfb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall recall: 69.3 precision: 61.4 f1: 65.1\n",
            "\t\ttp 1228\tfp 772\n",
            "\t\tfn 545\ttn 1455\n",
            "Masculine recall: 69.6 precision: 61.9 f1: 65.5\n",
            "\t\ttp 619\tfp 381\n",
            "\t\tfn 270\ttn 730\n",
            "Feminine recall: 68.9 precision: 60.9 f1: 64.6\n",
            "\t\ttp 609\tfp 391\n",
            "\t\tfn 275\ttn 725\n",
            "Bias (F/M): 0.99\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-330a7177ddfe>:115: DeprecationWarning: 'U' mode is deprecated\n",
            "  with open(filename, 'rU') as f:\n"
          ]
        }
      ]
    }
  ]
}